# ğŸŒ Modern Data Pipeline GCP

![Python](https://img.shields.io/badge/Python-3.9%2B-blue)
![Airflow](https://img.shields.io/badge/Airflow-2.x-blue)
![DBT](https://img.shields.io/badge/DBT-1.9.x-orange)
![GCP](https://img.shields.io/badge/GCP-ready-green)

---

## ğŸ“š Table of Contents:

- [ğŸ” Description](#-description)
- [ğŸš€ Project Overview](#-overview)
    - [ğŸ”„ Pipeline Highlights](#-pipeline-highlights)
    - [âœ… Features](#-features)
- [ğŸ“ `modern-data-pipeline-gcp` â€“ Project Root](#-modern-data-pipeline-gcptransformations--project-root)
- [ğŸ› ï¸ Tech Stack](#ï¸-tech-stack)
- [ğŸš€ Quick Start](#-quick-start)
    - [ğŸ“‹ Requirements](#-requirements)
    - [âš™ï¸ Setup & Usage](#ï¸-setup--usage)
        - [1. Clone & Navigate](#1-clone--navigate)
        - [2. Environment Setup](#2-environment-setup)
        - [3. Install Locally (optional)](#3-install-locally-optional)
        - [4. Start Services (Postgres + Airflow)](#4-start-services-postgres--airflow)
        - [5. Initialize DBT](#5-initialize-dbt)
        - [6. Run Pipeline](#6-run-pipeline)
- [ğŸ§ª Testing](#-testing)
- [ğŸ“¡ Monitoring & Logging](#-monitoring--logging)
- [ğŸš€ CI/CD](#-cicd)
- [ğŸ—ºï¸ Roadmap](#ï¸-roadmap)
- [ğŸ¤ Contributing Guidelines](#-contributing-guidelines)
    - [ğŸ§° How to Contribute](#-how-to-contribute)
    - [ğŸ§ª Testing](#-testing-1)
- [ğŸ§° Resources](#-resources)
    - [ğŸ§± DBT](#-dbt)
- [ğŸ‘©â€ğŸ’» Author](#-author)

---

## ğŸ” Description:

A production-grade, modular ETL workflow built with Apache Airflow, DBT, and Google Cloud Platform (GCP) services. Designed for orchestrated extraction, transformation, and loading, with integrated data quality, monitoring, and CI/CD best practices.

---

## ğŸš€ Project Overview:

This repository contains an end-to-end ETL workflow designed for scalability, maintainability, and cloud readiness. It showcases how to orchestrate data pipelines using Airflow, enrich and transform data with DBT, and deploy the solution using containerized environments and CI/CD pipelines.

#### ğŸ”„ Pipeline Highlights:

This repository delivers an end-to-end data pipeline with:
- **Extraction** from a PostgreSQL source and external exchange rate API  
- **Enrichment** and **Transformation** using DBT models  
- **Loading** to CSV, Google Sheets, and optionally to BigQuery  
- **Orchestration** via Airflow DAGs â€” modular and containerized  
- Built-in **data quality checks**, logging, and alerts

#### âœ… Features

- Modular structure: `extract`, `transform`, `load`, `validate`, `notify`  
- Docker + Docker Compose for consistent local execution  
- DBT for SQL-based modeling and schema tests  
- Airflow DAG to sequence the pipeline steps  
- Integration with GCP services: BigQuery, Sheets API, Secret Manager  
- Robust logging and optional Stackdriver integration

---

## ğŸ“ `modern-data-pipeline-gcp` â€“ Project Root:

```
.
â”œâ”€â”€ .venv                               # Virtual environment 
â”œâ”€â”€ logs
â”œâ”€â”€ transformations                     # Data transformation logic
â”‚   â”œâ”€â”€ .dbt
â”‚   â”‚   â”œâ”€â”€ users.yml
â”‚   â”‚   â””â”€â”€ profiles.yml                # DBT profiles (often symlinked or referenced)
â”‚   â”œâ”€â”€ analyses
â”‚   â”œâ”€â”€ data                            # CSV exports
â”‚   â”‚   â”œâ”€â”€ mock_order_items.csv
â”‚   â”‚   â”œâ”€â”€ mock_orders.csv
â”‚   â”‚   â”œâ”€â”€ mock_products.csv
â”‚   â”‚   â”œâ”€â”€ mock_rates.csv
â”‚   â”‚   â””â”€â”€ mock_users.csv
â”‚   â”œâ”€â”€ logs
â”‚   â”œâ”€â”€ macros
â”‚   â”œâ”€â”€ models                          # DBT models
â”‚   â”‚   â””â”€â”€ mock
â”‚   â”‚       â”œâ”€â”€ mock_order_items.sql
â”‚   â”‚       â”œâ”€â”€ mock_orders.sql
â”‚   â”‚       â”œâ”€â”€ mock_products.sql
â”‚   â”‚       â”œâ”€â”€ mock_users.sql
â”‚   â”‚       â””â”€â”€ schema.yml
â”‚   â”œâ”€â”€ scripts                         # Python scripts for pipeline steps
â”‚   â”‚   â”œâ”€â”€ export_tables.py
â”‚   â”‚   â”œâ”€â”€ load_exchange_rates.py
â”‚   â”‚   â””â”€â”€ push_to_bigquery.py
â”‚   â”œâ”€â”€ seeds
â”‚   â”œâ”€â”€ snapshots
â”‚   â”œâ”€â”€ target
â”‚   â”œâ”€â”€ tests
â”‚   â”œâ”€â”€ utils                           # Custom utility functions
|   |   â””â”€â”€ quality_checks.py
â”‚   â”œâ”€â”€ .env
â”‚   â”œâ”€â”€ .gitignore
â”‚   â”œâ”€â”€ dbt_project.yml                 # DBT project config
â”‚   â”œâ”€â”€ README.md
â”‚   â”œâ”€â”€ requirement-dev.txt             # Python dependencies
â”‚   â”œâ”€â”€ requirements.txt                # Python dependencies
â”‚   â””â”€â”€ run.sh
â”œâ”€â”€ .gitignore
â”œâ”€â”€ LICENSE
â””â”€â”€ README.md                           # Project documentation
```

---

## ğŸ› ï¸ Tech Stack:

| Layer            | Technologies                                  |
|------------------|-----------------------------------------------|
| Orchestration    | Airflow on Docker (locally) or Cloud Composer |
| Transformation   | DBT models deployed to BigQuery               |
| Source data      | PostgreSQL                                    |
| Destinations     | CSV, Google Sheets, BigQuery                  |
| Cloud infra      | GCP: BigQuery, Sheets API, Secret Manager     |
| Containerization | Docker & Docker Compose                       |
| Language         | Python                                        |
| CI/CD            | GitHub Actions                                |

---

## ğŸš€ Quick Start:

### ğŸ“‹ Requirements

- Docker & Dockerâ€¯Compose  
- Python 3.9+ (for local development)  
- GCP project with access to BigQuery, Sheets API, and Secret Manager  
- PostgreSQL instance for source data  
- `make` (optional, if using Makefile shortcuts)

---

### âš™ï¸ Setup & Usage

#### 1. Clone & Navigate

```
git clone https://github.com/CamilaJaviera91/modern-data-pipeline-gcp.git
cd modern-data-pipeline-gcp
```

#### 2. Environment Setup

Copy `.env.example` to `.env` and supply:

```
DBT_HOST=...
DBT_USER=...
DBT_PASSWORD=...
DBT_DBNAME=...
DBT_SCHEMA=...
DBT_PORT=...
EXCHANGE_API_KEY=...
GOOGLE_CREDENTIALS_PATH=...
BQ_PROJECT_ID=...
BQ_DATASET=...
```

#### 3. Install Locally (optional)

```
python3 -m venv .venv
source .venv/bin/activate
pip install -r requirements.txt
pip install -r requirements-dev.txt
```

#### 4. Start Services (Postgres + Airflow)

```
docker-compose up --build
```

#### 5. Initialize DBT

```
cd transformations
dbt init
```

#### 6. Run Pipeline
For a daily production run:
```
./run.sh run
```
Mock mode only:
```
./run.sh run --select enrich_exchange_rates
```

---

## ğŸ§ª Testing:

- **DBT tests** for schema, uniqueness, and relationships
- **Airflow DAG validation**: `airflow dags list`, `airflow dags test`
- **Unit tests** for custom Python functions in `/scripts` or `/dags`
- **CI pipeline** (planned): Linting, formatting, DAG validation, DBT compile

Run tests to validate your pipeline:

```
./run.sh test
# or
dbt test --select mock
```

---

## ğŸ“¡ Monitoring & Logging:

- Airflow task logs viewable via the web UI
- Custom loggers for API responses and ETL steps
- Optionally integrates with **Stackdriver Logging** and **Alerting**

---

## ğŸš€ CI/CD

- GitHub Actions triggers include:
  - Linting + formatting checks
  - DBT compilation and tests
  - Docker image builds
  - Deployment to Cloud Composer (planned)

---

## ğŸ—ºï¸ Roadmap:

- [ ] Add unit tests for transformation logic
- [ ] Set up CI/CD with GitHub Actions
- [ ] Auto-deploy to Cloud Composer
- [ ] Add support for other destinations (e.g., Snowflake, S3)

---

## ğŸ¤ Contributing Guidelines:

Thank you for your interest in contributing to this project!

#### ğŸ§° How to Contribute:

1. **Fork** the repository.
2. **Clone** your fork:  
   `git clone https://github.com/<your-username>/modern-data-pipeline-gcp.git`
3. Create a new branch:  
   `git checkout -b feature/your-feature-name`
4. Make your changes and **commit**:  
   `git commit -m "Add new feature"`
5. Push to your fork:  
   `git push origin feature/your-feature-name`
6. Submit a **pull request** to the `main` branch.

#### ğŸ§ª Testing:

Please run tests locally before submitting your PR:

```
pytest scripts/
dbt test
```

---

## ğŸ§° Resources:

### ğŸ§± DBT:

- [DBT Docs](https://docs.getdbt.com/docs/introduction)
- [DBT QA](https://discourse.getdbt.com/)
- [DBT Chat](https://community.getdbt.com/)
- [DBT Events](https://events.getdbt.com)
- [DBT blog](https://blog.getdbt.com/) 

---

## ğŸ‘©â€ğŸ’» Author:

**Camila Javiera MuÃ±oz Navarro**  
Data Engineer & Analyst | BigQuery | Airflow | Python | GCP  
[GitHub](https://github.com/CamilaJaviera91) | [LinkedIn](https://www.linkedin.com/in/camilajavieramn/) | [Portfolio](https://camilajaviera91.github.io/camila-portfolio/)

---

> â­ If you find this project useful, give it a â­ï¸ and share your feedback or ideas in [Issues](https://github.com/CamilaJaviera91/modern-data-pipeline-gcp/issues)!